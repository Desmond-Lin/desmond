## 1. 梯度爆炸和梯度消失
原因：
激活函数相关：反向传播的链式求导过程中，随着神经网络层数的增加，每层hidden layer都要叠加一次W*激活函数的导数。sigmoid函数的导数值最大是1/4，而每层的初
始化权重W通常小于1，这使得他们的乘积通常小于1。在层数过深的网络中，梯度消失就会发生。梯度爆炸的原因显而易见了，激活函数导数和权重乘积大于1.故激活函数通
常使用Relu函数，导数为1.另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。
